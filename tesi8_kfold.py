# -*- coding: utf-8 -*-
"""tesi8 kfold.ipynb

Automatically generated by Colaboratory.

"""

from pandas import concat
from google.colab import drive
drive.mount('/content/gdrive')

!unzip "./gdrive/My Drive/datasets/dat1.gz.zip"
!gunzip ./dat1.gz
!unzip "./gdrive/My Drive/datasets/dat2.gz.zip"
!gunzip ./dat2.gz

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jan 10 11:02:40 2019

@author: alessio
"""

#!/usr/bin/env python3
from sklearn.model_selection import train_test_split
import numpy as np

"""
questa funzione prende in ingresso un dataset e lo divide in segmenti
utili per una kfold. Ogni segmento viene a sua volta partizionato per 
l'80% in training set, il restante in test set
"""

def makeKfold(array, nFold):
	
    #inizializzo liste vuote da riutilizzare successivamanete
    trainingset = []
    validationset = []
    testset = []
    	
    lunghezzaFold = len(array) // nFold #prendo la lunghezza intera del dataset e definisco le lunghezze dei segmenti
	
    #per ogni passo segmento, creo training e test set
    for i in range(nFold):
        		
        	#se sono al primo passo, setto l'indice di divisione iniziale a zero
        if(i == 0):
            inizio = 0
		
    		#Seleziono il segmento che mi interessa
        divisione = array[inizio : lunghezzaFold * (i+1)]
		
        #divido il segmento selezionato in training set e test set    
        train, test = train_test_split(divisione, test_size=0.4)


        validation, test = train_test_split(test, test_size=0.5)

        #salvo i segmenti in una lista apposita
        trainingset.append(train)
        validationset.append(validation)
        testset.append(test)

        #per il segmento successivo, setto come indice iniziale quello finale del precedente
        inizio = lunghezzaFold * (i+1)

   
    trainingset = np.array(trainingset)
    validationset = np.array(validationset)
    testset = np.array(testset)
      
    return trainingset, validationset, testset

# -*- coding: utf-8 -*-
"""tesi6.ipynb

Automatically generated by Colaboratory.


"""


from pandas import concat

from pandas import read_csv #importo da pandas il lettore di csv
import numpy as np #importo la libreria numpy e la rinomino np
from dateutil import parser #parsa la data per una più facile resa del codice
from matplotlib import pyplot

inputFile1 = "./dat1" 

labels1 = read_csv(inputFile1, nrows=1, header=0) #legge il csv
labels1 = labels1.columns.tolist()  #prende solo le colonne
        
data1 = read_csv(inputFile1, header=0) #legge il csvd


inputFile2 = "./dat2" 

labels2 = read_csv(inputFile2, nrows=1, header=0) #legge il csv
labels2 = labels2.columns.tolist()  #prende solo le colonne
        
data2 = read_csv(inputFile2, header=0) #legge il csvd

print(data1)
print(data2)

#unisco i due dataset
result = concat([data1, data2], axis=1, join_axes=[data1.index])
print(result)

result

#elimino colonne non rilevanti
result = result.drop(['ID', 'countrycode'], axis=1)

result

array = np.array(result)

array[0]

for i in range(len(array)):
  
  if(array[i, -2] != 'Mobile' and array[i, -2] != 'Desktop'):
    
    if(array[i, -3] == 'Mozilla' or array[i, -3] == 'Mozilla Firefox' or 
      array[i, -3] == 'Chrome' or array[i, -3] == 'InternetExplorer' or 
      array[i, -3] == 'Internet Explorer' or array[i, -3] == 'Safari' or array[i, -3] == 'Edge'):
      
      array[i, -2] = 'Desktop'
      
    else:
      array[i, -2] = 'Mobile'

array[0]

import pandas as pd

array = pd.DataFrame(data=array[0:,0:])

print(array)

array = array.dropna() #elimino i campi NaN

print(array)

"""Così abbiamo salvato circa due milioni di righe"""

arraynp = np.array(array)

from collections import Counter

Counter(arraynp[:,-1])

#prendo un tot di label 0 per bilanciare il dataset

#nuovoarray = []

#for i in range(len(arraynp)):
#  if (arraynp[i, -1] == 0):
#    nuovoarray.append(arraynp[i])
    
#  if(len(nuovoarray) == 600000):
#    break

#len(nuovoarray)

#for i in range(len(arraynp)):
#  if (arraynp[i, -1] == 1):
#    nuovoarray.append(arraynp[i])
    
#print(len(nuovoarray))

#nuovoarray[700000]

#nuovoarray = np.array(nuovoarray)

#divido training e test set
from sklearn.model_selection import train_test_split

train, validation, test = makeKfold(arraynp, 3)

print(train)
print(validation)
print(test)

train[1]

trainlabels = train[:, :, -1]
testlabels = test[:, :, -1]

validationlabels = validation[:, :, -1]
validation = validation[:, :, 0:-1]

train = train[:, :, 0:-1]
test = test[:, :, 0:-1]

train[2]

#converto i valori stringa in valori numerici per la rete neurale 

def numerizza(arraynp):

  for i in range(len(arraynp)):
    arraynp[i,0] = arraynp[i,0].replace("-", "")
    arraynp[i,0] = arraynp[i,0].replace(" ", "")
    arraynp[i,0] = arraynp[i,0].replace(":", "")


    if (arraynp[i,-1] == 'Desktop'):
      arraynp[i,-1] = 7
    elif (arraynp[i,-1] == 'Mobile'):
      arraynp[i,-1] = 8
    else: 
      arraynp[i,-1] = 9

    if (arraynp[i, -2] == 'Mozilla Firefox' or arraynp[i, -2] == 'Mozilla' or arraynp[i, -2] == 'Firefox'):
      arraynp[i,-2] = 6
    elif (arraynp[i,-2] == 'Google Chrome' or arraynp[i,-2] == 'Chrome'):
      arraynp[i,-2] = 1 
    elif (arraynp[i,-2] == 'InternetExplorer' or arraynp[i,-2] == 'Internet Explorer' or arraynp[i,-2] == 'IE'):
      arraynp[i,-2] = 2
    elif (arraynp[i,-2] == 'Opera'):
      arraynp[i,-2] = 3
    elif (arraynp[i,-2] == 'Safari'):
      arraynp[i,-2] = 4
    elif (arraynp[i,-2] == 'Edge'):
      arraynp[i,-2] = 5

  return arraynp


for i in range(3):
  test[i] = numerizza(test[i])
  train[i] = numerizza(train[i])
  validation[i] = numerizza(validation[i])

  
  
print(test[0])

#importo una libreria per trasformare i dati in un range da [0,1]
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1)) #seleziono il range in cui scalare i dati

for i in range(3):
  train[i] = scaler.fit_transform(train[i]) #scalo i dati nel range selezionato
  test[i] = scaler.fit_transform(test[i]) #scalo i dati nel range selezionato
  validation[i] = scaler.fit_transform(validation[i])

print(train[0]) #controllo se sia andata a buon fine




from sklearn.model_selection import KFold
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Flatten, Embedding
from keras.optimizers import Adam, RMSprop, SGD
from pandas import DataFrame
from keras import regularizers
from keras.callbacks import ModelCheckpoint, History, EarlyStopping
import time
from matplotlib import pyplot
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.preprocessing import MinMaxScaler

"""
Da editare
"""

class ReteNeurale:
  
  def avvioRete(self, traindata, trainlabels, validationData, validationLabels):
              
    timestr = time.strftime("%Y%m%d-%H%M%S") #l'ora attuale dell'esecuzione
    
    model = Sequential()
    model.add(Dense(16, input_dim=7, activation='relu', kernel_initializer='random_uniform',bias_initializer='zeros'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
    adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])
    # Fit the model        
    history = model.fit(traindata, 
                        trainlabels, 
                        epochs=700,
                        batch_size=256,
                        verbose=0,
                        validation_data=(validationData, validationLabels))
    
    self.faiplot(history, timestr)
    
    return model, history, timestr
    
    
  def faiplot(self, history, timestr):
    
    cartella = "./" 

    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='validation')
    pyplot.title('Loss')
    pyplot.xlabel('Epochs')
    pyplot.legend()
    pyplot.savefig(cartella + 'Loss_' + timestr + '.png')
    pyplot.show()

    pyplot.plot(history.history['acc'], label='train')
    pyplot.plot(history.history['val_acc'], label='validation')
    pyplot.title('Accuracy')
    pyplot.xlabel('Epochs')
    pyplot.legend()
    pyplot.savefig(cartella + 'Accuracy_' + timestr + '.png')
    pyplot.show()

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

for i in range(3):
  
  print("Rete neurale " + str(i))

  nn = ReteNeurale()
  model, history, timestring = nn.avvioRete(train[i], trainlabels[i], validation[i], validationlabels[i])

  #predizioni sul test
  predicted_probs = model.predict(test[i]) #qui predico le probabilità per riga
  predicted_labels = [int(round(x[0])) for x in predicted_probs] #etichettatura per riga

  print(predicted_labels)
  precisione0 = precision_score(testlabels[i].tolist(), predicted_labels)
  recupero0 = recall_score(testlabels[i].tolist(), predicted_labels)
  f1_0 = f1_score(testlabels[i].tolist(), predicted_labels)
  accuracy = accuracy_score(testlabels[i].tolist(), predicted_labels)

  print(accuracy)
  print(precisione0)
  print(recupero0)
  print(f1_0)

!ls

#importo una libreria per trasformare i dati in un range da [0,1]
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1)) #seleziono il range in cui scalare i dati

for i in range(3):
  train[i] = scaler.fit_transform(train[i]) #scalo i dati nel range selezionato
  test[i] = scaler.fit_transform(test[i]) #scalo i dati nel range selezionato
  validation[i] = scaler.fit_transform(validation[i])

print(train[0]) #controllo se sia andata a buon fine




from sklearn.model_selection import KFold
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Flatten, Embedding
from keras.optimizers import Adam, RMSprop, SGD
from pandas import DataFrame
from keras import regularizers
from keras.callbacks import ModelCheckpoint, History, EarlyStopping
import time
from matplotlib import pyplot
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.preprocessing import MinMaxScaler

"""
Da editare
"""

class ReteNeurale:
  
  def avvioRete(self, traindata, trainlabels, validationData, validationLabels):
              
    timestr = time.strftime("%Y%m%d-%H%M%S") #l'ora attuale dell'esecuzione
    
    model = Sequential()
    model.add(Dense(16, input_dim=7, activation='relu', kernel_initializer='random_uniform',bias_initializer='zeros'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
    adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])
    # Fit the model        
    history = model.fit(traindata, 
                        trainlabels, 
                        epochs=300,
                        batch_size=256,
                        verbose=0,
                        validation_data=(validationData, validationLabels))
    
    self.faiplot(history, timestr)
    
    return model, history, timestr
    
    
  def faiplot(self, history, timestr):
    
    cartella = "./" 

    pyplot.plot(history.history['loss'], label='train')
    pyplot.plot(history.history['val_loss'], label='validation')
    pyplot.title('Loss')
    pyplot.xlabel('Epochs')
    pyplot.legend()
    pyplot.savefig(cartella + 'Loss_' + timestr + '.png')
    pyplot.show()

    pyplot.plot(history.history['acc'], label='train')
    pyplot.plot(history.history['val_acc'], label='validation')
    pyplot.title('Accuracy')
    pyplot.xlabel('Epochs')
    pyplot.legend()
    pyplot.savefig(cartella + 'Accuracy_' + timestr + '.png')
    pyplot.show()

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

for i in range(3):
  
  print("Rete neurale " + str(i))

  nn = ReteNeurale()
  model, history, timestring = nn.avvioRete(train[i], trainlabels[i], validation[i], validationlabels[i])

  #predizioni sul test
  predicted_probs = model.predict(test[i]) #qui predico le probabilità per riga
  predicted_labels = [int(round(x[0])) for x in predicted_probs] #etichettatura per riga

  print(predicted_labels)
  precisione0 = precision_score(testlabels[i].tolist(), predicted_labels)
  recupero0 = recall_score(testlabels[i].tolist(), predicted_labels)
  f1_0 = f1_score(testlabels[i].tolist(), predicted_labels)
  accuracy = accuracy_score(testlabels[i].tolist(), predicted_labels)

  print(accuracy)
  print(precisione0)
  print(recupero0)
  print(f1_0)

!ls

